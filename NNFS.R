# Neural Net from scratch
# Robert Kirk DeLisle
# 2018.07.05
#

#######################
## Activation functions
#######################

linear = function(z) {
  z
}

sigmoid = function(z) {
  1/(1+exp(-z))
}

relu = function(z) {
  z[which(z<0)] = 0
  z
}

softmax = function(z) {
  exp(z)/sum(exp(z))
}

# tanh is already defined in R

######################################
## Derivatives of activation functions
######################################

d.linear = function() {
  1
}

d.sigmoid = function(a) {
  # a is the activation of the layer of interest
  # in other words, it is a = sigmoid(z)
  # derivative can be achieved with simple calculation if
  # the original activations are cached
  
  a * (1-a)
}

d.tahn = function(a) {
  # a is the activation of the layer of interest
  # in other words, it is a = tanh(z)
  # derivative can be achieved with simple calculation if
  # the original activations are cached
  
  1 - a*a
}

d.softmax = function(a, y) {
  # a = the set of activations at the softmax layer
  # y = the target classes as one-hot vector
  # assumed that the loss function is the log loss = -sum(y(i) * log(p(i)))
  # with p(i) = softmax(z)
  
  a - y 
}

d.relu = function(a) {
  # a is the activation of the layer of interest
  
  z = rep(0,length(a))
  z[ which(z>0) ] = 1
  z
}

###############
# Define the NN
###############

NNModel = function( input.dim = NULL, layers = NULL, activations = NULL, 
                    batch.norm = NULL, drop.out.rate = NULL,
                    learning.rate = 0.01 ) {
  
  # input.dim = dimensions of input
  # layers = vector of number of nodes per hidden/output layer
  # activations = vector of activations types per layer
  #               can be linear, tanh, sigmoid, relu, and softmax
  #               softmax is assumed to be the last layer for multicategory output
  # batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
  # drop.out.rate = vector of values for drop-out rate per layer
  # assumed all vectors same length = number of layers
  
  # returns a list structured for later computations
  
  nn = list( layers = list(), learning.rate = learning.rate)
  
  nn$layers = lapply( 1:(length(layers)), function(l) {
    if( l == 1){
      weights = matrix( rnorm( input.dim*layers[l], 0, 0.01), 
                        nrow = input.dim )
    }else{
      weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01), 
                        nrow = layers[l-1] )
    }
    list( activation = activations[l],
          weights = weights)
  })
  
  names(nn$layers) = paste('L', 1:(length(layers)), sep='')
  nn
}

#################
# Model functions
#################

forward.prop = function(NNmod = NULL, X=NULL){
  
  # NNmod = list generated from NNmod() function above
  # X = matrix of inputs (1 x m) of appropriate dimensions for the NNmod
  #     effectively one row from the training matrix
  
  # returns a list of the outputs generated at each layer
  #         for use in back propagation
  
  layers = names(NNmod$layers)
  #lout = list()
  
  for(l in 1:length(layers)){
    
    if( l == 1 ){
      z = X %*% NNmod$layers[[layers[l]]]$weights
    }else{
      z = lout[[layers[l-1]]] %*% NNmod$layers[[layers[l]]]$weights
    }
    
    if( NNmod$layers[[layers[l]]]$activation == 'linear' ){
      NNmod$layers[[layers[l]]]$a = linear(z)
    }else if( NNmod$layers[[layers[l]]]$activation == 'sigmoid' ){
      NNmod$layers[[layers[l]]]$a = sigmoid(z)
    }else if( NNmod$layers[[layers[l]]]$activation == 'relu' ){
      NNmod$layers[[layers[l]]]$a = relu(z)
    }else if( NNmod$layers[[layers[l]]]$activation == 'tanh' ){
      NNmod$layers[[layers[l]]]$a = tanh(z)
    }else if( NNmod$layers[[layers[l]]]$activation == 'softmax' ){
      NNmod$layers[[layers[l]]]$a = softmax(z)
    }
  }
  
  NNmod
}

back.prop = function(NNmod=NULL, layer.outputs=NULL, expected.outputs=NULL) {
  
  # NNModel = list generated from NNModel() function above
  # layer.outputs = list generated by forward.prop on most recent training step
  # expected.outputs = vector of the actual y-values being modeled
  
  # returns a NNModel list with updated weights
  
}

train = function(NNmod=NULL, X=NULL, mini.batch.size=NULL, epochs=NULL) {
  
  # NNModel =list generated from NNModel() function above
  # X = the full training dataset organized as observations X covariates (nXm)
  # mini.batch.size = number of observations used in each training step
  # epochs = number of total passes through the dataset
  
  # returns a NNModel list with trained weights
  
  batches = ceiling(dim(X)[1]/mini.batch.size)
    
  for( e in 1:epochs ){
    mini.batch.order = sample(dim(X)[1])
    for( mb in 0:(batches-1) ){
      mb.start = (mini.batch.size*mb+1)
      mb.stop = min((mini.batch.size*(mb+1)), dim(X)[1])
      mini.batch = X[ mb.start:mb.stop, ]
      
      NNmod = forward.prop(NNmod, X=mini.batch)
      
      
    }
  }
  NNmod
}

predict = function( NNModel=NULL, X=NULL) {
  
  # NNModel =list generated from NNModel() function above
  # X = matrix the full training dataset organized as observations X covariates (nXm)
  
  # returns a vector of predictions for each observation in the X set
  # assume input dimensions and order of covariates are consistent with the
  # supplied model
  
}

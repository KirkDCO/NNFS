fs.res = Search(fs.mcp, num.cores=4)
library(doMC)
fs.res = Search(fs.mcp, num.cores=4)
plot(fs.res)
fs.res
WorldPhones
source("~/WorkingData/BI_git/sandbox/kdelisle/Essentials/SpecialtyColors.R")
col.string = fg.col.string
plot(fs.res)
fs.res$candidate.markers
fs.res$cross.val$Run1
?round
installed.packages()
z = installed.packages()
class(z)
z=data.frame(z)
z$License
z$License_is_FOSS
z$License_restricts_use
unique(z$License)
sort(unique(z$License))
which(z$License == 'Artistic-2.0')
z[15,]
library(psych)
install.packages('psych')
library(psych)
?principal
principal
isCorrelation
isSymmetric()
isSymmetric
isSymmetric
?isSymmetric
a = matrix(c(.66,4.39,0.89,-1.18), nrow=4)
m1.m2 = matrix(c(-.07-.24, -0.08-.06, 1.37-2.59, .44-.43), nrow=4)
s=c(0.05,0.02,0.06,0,0.02,0.01,-.02,0,0.06,0.02,0.65,0.03, 0,0,0,0.03,0.03), nrow=4)
s=matrix(c(0.05,0.02,0.06,0,0.02,0.01,-.02,0,0.06,0.02,0.65,0.03, 0,0,0,0.03,0.03), nrow=4)
s=matrix(c(0.05,0.02,0.06,0,0.02,0.01,-.02,0,0.06,0.02,0.65,0.03, 0,0.03,0.03), nrow=4)
s=matrix(c(0.05,0.02,0.06,0,0.02,0.01,-.02,0,0.06,0.02,0.65,0.03, 0,0,.03,0.03), nrow=4)
s
m1pm2 = matrix(c(-.07+.24, -0.08+.06, 1.37+2.59, .44+.43), nrow=4)
t(m1.m2) %*% solve(s) %*% m1pm2
a
m1 = matrix(c(-.07, -.08, 1.37, .44), nrow=4)
m2 = matrix(c(.24, 0.06, 2.59, 0.43), nrow=4)
m1
m2
( t(a) %*% m1 + t(a) %*% m2)/2
0.84^2 + 0.38^2
a = installed.packages()
a
a$License
class(a)
a = data.frame(a)
a$License
unique(a$License)
sort(unique(a$License))
sort(unique(a$License))
ept = rnorm(1000, 0,1)
dat = matrix( rnorm(5000000, 0,1), nrow = 1000)
dim(dat)
?lm
res = lm(ept~col[,1])
res = lm(ept~dat[,1])
res
summary(res)
s = summary(res)
s$cov.unscaled
s$adj.r.squared
s$r.squared
s$fstatistic
s[1]
}
uni = apply(dat, 2, function(col) {
s = summary(lm(ept ~ col))
c('rsq' = s$r.squared,
'p.val' = pf(s$fstatistic[1], s$fstatistic[2], s$fstatistic[3], lower.tail = FALSE)
)
})
class(uni)
uni
uni = data.frame(apply(dat, 2, function(col) {
s = summary(lm(ept ~ col))
c('rsq' = s$r.squared,
'p.val' = pf(s$fstatistic[1], s$fstatistic[2], s$fstatistic[3], lower.tail = FALSE)
)
}))
uni
uni[1,]
uni[2,]
uni$X1
rownames(uni)
uni = t(data.frame(apply(dat, 2, function(col) {
s = summary(lm(ept ~ col))
c('rsq' = s$r.squared,
'p.val' = pf(s$fstatistic[1], s$fstatistic[2], s$fstatistic[3], lower.tail = FALSE)
)
})))
uni
colnames(uni)
uni$rsq
class(uni)
uni = data.frame(t(apply(dat, 2, function(col) {
s = summary(lm(ept ~ col))
c('rsq' = s$r.squared,
'p.val' = pf(s$fstatistic[1], s$fstatistic[2], s$fstatistic[3], lower.tail = FALSE)
)
})))
uni$rsq
max(uni$rsq)
min(uni$rsq)
histogram(uni$rsq, breaks=50)
hist(uni$rsq, breaks=50)
max(uni$p.val.value)
min(uni$p.val.value)
uni = uni[order(uni$p.val.value), ]
head(uni)
library(glmnet)
install.packages("glmnet")
library(glmnet)
grp = sample(c(1,2,3,4,5), size=1000, replace=TRUE)
grp
table(grp)
g=1
trn.x = dat[ -which(grp==g), ]
trn.y = ept[ -which(grp==g), ]
trn.y = ept[ -which(grp==g)]
tst.x = dat[ which(grp==g), ]
tst.y = ept[ which(grp==g)]
m = glmnet(trn.x, trn.y, family='gaussian')
m
summary(m)
m$beta
m$lambda
plot(m)
m$a0
?glmnet
l = cv.glmnet(trn.x, trn.y, family='gaussian')$lambda.1se
l
m = glmnet(trn.x, trn.y, family='gaussian', lambda=l)
m
m$beta
coeff(m)
coef(m0)
coef(m)
plot(coef(m))
l = cv.glmnet(trn.x, trn.y, family='gaussian')
l$lambda
l$lambda.min
l = cv.glmnet(trn.x, trn.y, family='gaussian')$lambda.min
m = glmnet(trn.x, trn.y, family='gaussian', lambda=l)
plot(coef(m))
predict(m, tst.x)
p = predict(m, tst.x)
plot(p, txt.y)
plot(p, tst.y)
uni
head(uni)
setwd("/storage/Temp/NNFS")
source('/storage/Temp/NNFS/NNFS.R')
source('/storage/Temp/NNFS/NNFS.R')
X = matrix(rnorm(200,0,1), nrow=100)
X
Y = matrix(rnorm(100,0,1), nrow=100)
Y
nn = NNModel(X, Y, X, Y, layers=c(1), activations='linear')
nn
source('/storage/Temp/NNFS/NNFS.R')
nn = NNModel(X, Y, X, Y, layers=c(1), activations='linear')
nn = NNModel(input.dim=2, layers=c(1), activations='linear')
nn
nn.trn = train(NNmod=nn, X.trn = X, Y.trn = Y, mini.batch.size = 15, epochs=1)
nn.trn
reverse()
10:1
debugSource('/storage/Temp/NNFS/NNFS.R')
nn.trn = train(NNmod=nn, X.trn = X, Y.trn = Y, mini.batch.size = 15, epochs=1)
expected.outputs
debugSource('/storage/Temp/NNFS/NNFS.R')
nn.trn = train(NNmod=nn, X.trn = X, Y.trn = Y, mini.batch.size = 15, epochs=1)
expected.outputs
NNmod$layers[[layers[l]]]$a)
NNmod$layers[[layers[l]]]$a
debugSource('/storage/Temp/NNFS/NNFS.R')
nn.trn = train(NNmod=nn, X.trn = X, Y.trn = Y, mini.batch.size = 15, epochs=1)
mini.batch.X
mini.batch.Y
Y.trn
debugSource('/storage/Temp/NNFS/NNFS.R')
nn.trn = train(NNmod=nn, X.trn = X, Y.trn = Y, mini.batch.size = 15, epochs=1)
mini.batch.Y
grad
grad
NNmod$layers[[layer]]$weights
debugSource('/storage/Temp/NNFS/NNFS.R')
nn.trn = train(NNmod=nn, X.trn = X, Y.trn = Y, mini.batch.size = 15, epochs=1)
debugSource('/storage/Temp/NNFS/NNFS.R')
nn.trn = train(NNmod=nn, X.trn = X, Y.trn = Y, mini.batch.size = 15, epochs=1)
avg.error
s = sample(1:100, replace = TRUE, size=100)
s
z = sapply(1:1000, max(sample(1:100, replace = TRUE, size=100)))
z = sapply(1:1000, {max(sample(1:100, replace = TRUE, size=100))})
z = sapply(1:1000, function() {max(sample(1:100, replace = TRUE, size=100))})
z = sapply(1:1000, function(a) {max(sample(1:100, replace = TRUE, size=100))})
z
histogram (z)
hist(z)
z = sapply(1:1000, function(a) {max(sample(1:50, replace = TRUE, size=100))})
hist(z)
z = sapply(1:1000, function(a) {mean(sample(1:50, replace = TRUE, size=100))})
hist(z)
library(MASS) #for mvrnorm
nn = NNModel(input.dim=2, layers=c(7,7,1), activations=c('relu','relu','sigmoid'))
debugSource('/storage/Temp/NNFS/NNFS.R')
source('/storage/Temp/NNFS/NNFS.R')
library(MASS) #for mvrnorm
nn = NNModel(input.dim=2, layers=c(7,7,1), activations=c('relu','relu','sigmoid'))
X.trn = rbind( mvrnorm(50, mu=c(2,4), Sigma = diag(1,nrow=2,ncol=2)),
mvrnorm(50, mu=c(4,2), Sigma = diag(1,nrow=2,ncol=2)))
Y.trn = matrix( c(rep(0,50), rep(1,50)) )
nn.prd = predict(nn.trn, X.trn)
nn.trn = nn
nn.prd = predict(nn.trn, X.trn)
Y.trn - nn.prd
nn.trn = train(nn,X.trn,Y.trn, epochs=1000, mini.batch.size=5, learning.rate=0.5)
nn.prd = predict(nn.trn, X.trn)
Y.trn - nn.prd
nn.prd
#plot clsses and decision boundary
plot(X.trn[,1], X.trn[,2], col=c('red','blue')[Y.trn+1],
pch=19)
cut.point = .5
for( x1 in seq(from=min(X.trn[,1]), to=max(X.trn[,1]), length.out=100) ) {
for( x2 in seq(from=min(X.trn[,2]), to=max(X.trn[,2]), length.out=100) ){
prd = predict(nn.trn, matrix(c(x1,x2), nrow=1))
clr = c('red', 'blue')[ (prd>cut.point)+1 ]
points(x1,x2,pch=19,cex=.25,col=clr)
}
}
source('/storage/Temp/NNFS/NNFS.R')
# 3-layer leaky relu classification, clusters
#############################################
library(MASS)
nn = NNModel(input.dim = 2, layers=c(2,1), activation=c('relu','sigmoid'))
X.trn = rbind( mvrnorm(75, mu=c(1,2), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(75, mu=c(3,4), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(100, mu=c(2,3), Sigma = diag(5,nrow=2,ncol=2)))
Y.trn = matrix( c(rep(0,150), rep(1,100)) )
nn.trn = train(nn,X.trn,Y.trn, epochs=10000, mini.batch.size=15, learning.rate=0.05)
#plot classes and decision boundary
plot(X.trn[,1], X.trn[,2], col=c(rep('red',150),rep('blue',100)),
pch=19)
cut.point = .5
for( x1 in seq(from=min(X.trn[,1]), to=max(X.trn[,1]), length.out=200) ) {
for( x2 in seq(from=min(X.trn[,2]), to=max(X.trn[,2]), length.out=200) ){
prd = predict(nn.trn, matrix(c(x1,x2), nrow=1))
clr = c('red', 'blue')[ (prd>cut.point)+1 ]
points(x1,x2,pch=19,cex=.15,col=clr)
}
}
source('/storage/Temp/NNFS/NNFS.R')
# 3-layer leaky relu classification, clusters
#############################################
library(MASS)
nn = NNModel(input.dim = 2, layers=c(3,1), activation=c('leaky.relu','sigmoid'))
X.trn = rbind( mvrnorm(75, mu=c(1,2), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(75, mu=c(3,4), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(100, mu=c(2,3), Sigma = diag(5,nrow=2,ncol=2)))
Y.trn = matrix( c(rep(0,150), rep(1,100)) )
nn.trn = train(nn,X.trn,Y.trn, epochs=10000, mini.batch.size=15, learning.rate=0.05)
nn.prd = predict(nn.trn, X.trn)
Y.trn - nn.prd
#plot classes and decision boundary
plot(X.trn[,1], X.trn[,2], col=c(rep('red',150),rep('blue',100)),
pch=19)
cut.point = .5
for( x1 in seq(from=min(X.trn[,1]), to=max(X.trn[,1]), length.out=200) ) {
for( x2 in seq(from=min(X.trn[,2]), to=max(X.trn[,2]), length.out=200) ){
prd = predict(nn.trn, matrix(c(x1,x2), nrow=1))
clr = c('red', 'blue')[ (prd>cut.point)+1 ]
points(x1,x2,pch=19,cex=.15,col=clr)
}
}
X.trn = rbind( mvrnorm(75, mu=c(1,2), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(75, mu=c(3,4), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(150, mu=c(2,3), Sigma = diag(5,nrow=2,ncol=2)))
#plot classes and decision boundary
plot(X.trn[,1], X.trn[,2], col=c(rep('red',150),rep('blue',100)),
pch=19)
# 3-layer leaky relu classification, clusters
#############################################
library(MASS)
nn = NNModel(input.dim = 2, layers=c(3,1), activation=c('leaky.relu','sigmoid'))
X.trn = rbind( mvrnorm(75, mu=c(1,2), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(75, mu=c(3,4), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(150, mu=c(2,3), Sigma = diag(5,nrow=2,ncol=2)))
Y.trn = matrix( c(rep(0,150), rep(1,150)) )
#plot classes and decision boundary
plot(X.trn[,1], X.trn[,2], col=c(rep('red',150),rep('blue',150)),
pch=19)
nn.trn = train(nn,X.trn,Y.trn, epochs=10000, mini.batch.size=15, learning.rate=0.05)
nn.prd = predict(nn.trn, X.trn)
Y.trn - nn.prd
cut.point = .5
for( x1 in seq(from=min(X.trn[,1]), to=max(X.trn[,1]), length.out=200) ) {
for( x2 in seq(from=min(X.trn[,2]), to=max(X.trn[,2]), length.out=200) ){
prd = predict(nn.trn, matrix(c(x1,x2), nrow=1))
clr = c('red', 'blue')[ (prd>cut.point)+1 ]
points(x1,x2,pch=19,cex=.15,col=clr)
}
}
setwd("/storage/Temp/NNFS")
source('/storage/Temp/NNFS/NNFS.R')
debugSource('/storage/Temp/NNFS/NNFS.R')
# 2-layer leaky relu classification, clusters
#############################################
library(MASS)
nn = NNModel(input.dim = 2, layers=c(3,1), activation=c('leaky.relu','sigmoid'))
X.trn = rbind( mvrnorm(75, mu=c(1,2), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(75, mu=c(3,4), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(150, mu=c(2,3), Sigma = diag(5,nrow=2,ncol=2)))
Y.trn = matrix( c(rep(0,150), rep(1,150)) )
nn.trn = train(nn,X.trn,Y.trn, epochs=10000, mini.batch.size=15, learning.rate=0.05)
# compute deltas
if( l == length(layers) ){  #we're at the output layer
if( NNmod.old$layers[[layer]]$activation == 'softmax'){
err = error(Y.trn, NNmod.old$layers[[layer]]$z)
dummy = matrix(1, ncol=ncol(err), nrow=nrow(err))
delta = t(mapply(function(e, z) {
print(z)
print(e)
wt.del = z %*% e
}, err, split(dummy,row(dummy), drop=FALSE),
SIMPLIFY=FALSE))
}else{
delta = lapply( error(Y.trn, NNmod.old$layers[[layer]]$z), function(v) {v})
}
}else{
next.layer = layers[l+1]
delta = t(mapply(function(del, z) {
print(del)
print(z)
wt.del = NNmod.old$layers[[next.layer]]$weights %*% del
wt.del * d(as.matrix(z, nrow=dim(wt.del)[1], ncol=dim(wt.del[2])))},
delta, split(NNmod.old$layers[[layer]]$z,
row(NNmod.old$layers[[layer]]$z), drop=FALSE),
SIMPLIFY=FALSE))
}
debugSource('/storage/Temp/NNFS/NNFS.R')
# 2-layer leaky relu classification, clusters
#############################################
library(MASS)
nn = NNModel(input.dim = 2, layers=c(3,1), activation=c('leaky.relu','sigmoid'))
X.trn = rbind( mvrnorm(75, mu=c(1,2), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(75, mu=c(3,4), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(150, mu=c(2,3), Sigma = diag(5,nrow=2,ncol=2)))
Y.trn = matrix( c(rep(0,150), rep(1,150)) )
nn.trn = train(nn,X.trn,Y.trn, epochs=10000, mini.batch.size=15, learning.rate=0.05)
debugSource('/storage/Temp/NNFS/NNFS.R')
debugSource('/storage/Temp/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=10000, mini.batch.size=15, learning.rate=0.05)
# 3-layer multinomial, 3 clusters
######################################################
library(MASS)
nn = NNModel(input.dim = 2, layers=c(5,5,3), activation=c('sigmoid','sigmoid','softmax'))
X.trn = rbind( mvrnorm(75, mu=c(2,0), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(75, mu=c(6,4), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(75, mu=c(2,6), Sigma = diag(.25,nrow=2,ncol=2)))
Y.trn = matrix( c(rep(c(1,0,0), 75),
rep(c(0,1,0), 75),
rep(c(0,0,1), 75)), ncol=3, byrow=TRUE )
nn.trn = train(nn,X.trn,Y.trn, epochs=50000, mini.batch.size=15, learning.rate=0.1)
drr
err
dummy
delta
debugSource('/storage/Temp/NNFS/NNFS.R')
debugSource('/storage/Temp/NNFS/NNFS.R')
# 2-layer leaky relu classification, clusters
#############################################
library(MASS)
nn = NNModel(input.dim = 2, layers=c(3,1), activation=c('leaky.relu','sigmoid'))
X.trn = rbind( mvrnorm(75, mu=c(1,2), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(75, mu=c(3,4), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(150, mu=c(2,3), Sigma = diag(5,nrow=2,ncol=2)))
Y.trn = matrix( c(rep(0,150), rep(1,150)) )
nn.trn = train(nn,X.trn,Y.trn, epochs=10000, mini.batch.size=15, learning.rate=0.05)
delta
delta
# 3-layer sigmoid classification, 3 contained clusters
######################################################
library(MASS)
nn = NNModel(input.dim = 2, layers=c(5,5,5,1), activation=c('sigmoid','sigmoid','sigmoid','sigmoid'))
X.trn = rbind( mvrnorm(75, mu=c(2,0), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(75, mu=c(6,4), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(75, mu=c(2,6), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(200, mu=c(3,3), Sigma = diag(5,nrow=2,ncol=2)))
Y.trn = matrix( c(rep(0,225), rep(1,200)) )
nn.trn = train(nn,X.trn,Y.trn, epochs=50000, mini.batch.size=15, learning.rate=0.1)
delta
delta
delta
debugSource('/storage/Temp/NNFS/NNFS.R')
# 3-layer multinomial, 3 clusters
######################################################
library(MASS)
nn = NNModel(input.dim = 2, layers=c(5,5,3), activation=c('sigmoid','sigmoid','softmax'))
X.trn = rbind( mvrnorm(75, mu=c(2,0), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(75, mu=c(6,4), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(75, mu=c(2,6), Sigma = diag(.25,nrow=2,ncol=2)))
Y.trn = matrix( c(rep(c(1,0,0), 75),
rep(c(0,1,0), 75),
rep(c(0,0,1), 75)), ncol=3, byrow=TRUE )
nn.trn = train(nn,X.trn,Y.trn, epochs=50000, mini.batch.size=15, learning.rate=0.1)
err
dummy
?mapply
debugSource('/storage/Temp/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=50000, mini.batch.size=15, learning.rate=0.1)
debugSource('/storage/Temp/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=50000, mini.batch.size=15, learning.rate=0.1)
debugSource('/storage/Temp/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=50000, mini.batch.size=15, learning.rate=0.1)
delta
delta[[1]]
debugSource('/storage/Temp/NNFS/NNFS.R')
debugSource('/storage/Temp/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=50000, mini.batch.size=15, learning.rate=0.1)
delta
matrix(1,1,1)
debugSource('/storage/Temp/NNFS/NNFS.R')
debugSource('/storage/Temp/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=50000, mini.batch.size=15, learning.rate=0.1)
debugSource('/storage/Temp/NNFS/NNFS.R')
debugSource('/storage/Temp/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=50000, mini.batch.size=15, learning.rate=0.1)
debugSource('/storage/Temp/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=50000, mini.batch.size=15, learning.rate=0.1)
err
debugSource('/storage/Temp/NNFS/NNFS.R')
debugSource('/storage/Temp/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=50000, mini.batch.size=15, learning.rate=0.1)
debugSource('/storage/Temp/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=50000, mini.batch.size=15, learning.rate=0.1)
debugSource('/storage/Temp/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=50000, mini.batch.size=15, learning.rate=0.1)
delta
delta[[1]]
source('/storage/Temp/NNFS/NNFS.R')
source('/storage/Temp/NNFS/NNFS.R')
# 3-layer multinomial, 3 clusters
######################################################
library(MASS)
nn = NNModel(input.dim = 2, layers=c(5,5,3), activation=c('sigmoid','sigmoid','softmax'))
X.trn = rbind( mvrnorm(75, mu=c(2,0), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(75, mu=c(6,4), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(75, mu=c(2,6), Sigma = diag(.25,nrow=2,ncol=2)))
Y.trn = matrix( c(rep(c(1,0,0), 75),
rep(c(0,1,0), 75),
rep(c(0,0,1), 75)), ncol=3, byrow=TRUE )
nn.trn = train(nn,X.trn,Y.trn, epochs=1000, mini.batch.size=15, learning.rate=0.1)
nn.prd = predict(nn.trn, X.trn)
Y.trn - nn.prd
nn.trn
# 2-layer leaky relu classification, clusters
#############################################
library(MASS)
nn = NNModel(input.dim = 2, layers=c(3,1), activation=c('leaky.relu','sigmoid'))
X.trn = rbind( mvrnorm(75, mu=c(1,2), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(75, mu=c(3,4), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(150, mu=c(2,3), Sigma = diag(5,nrow=2,ncol=2)))
Y.trn = matrix( c(rep(0,150), rep(1,150)) )
nn.trn = train(nn,X.trn,Y.trn, epochs=10000, mini.batch.size=15, learning.rate=0.05)
nn.prd = predict(nn.trn, X.trn)
Y.trn - nn.prd
#plot classes and decision boundary
plot(X.trn[,1], X.trn[,2], col=c(rep('red',150),rep('blue',150)),
pch=19)
cut.point = .5
for( x1 in seq(from=min(X.trn[,1]), to=max(X.trn[,1]), length.out=200) ) {
for( x2 in seq(from=min(X.trn[,2]), to=max(X.trn[,2]), length.out=200) ){
prd = predict(nn.trn, matrix(c(x1,x2), nrow=1))
clr = c('red', 'blue')[ (prd>cut.point)+1 ]
points(x1,x2,pch=19,cex=.15,col=clr)
}
}
debugSource('/storage/Temp/NNFS/NNFS.R')
# 3-layer multinomial, 3 clusters
######################################################
library(MASS)
nn = NNModel(input.dim = 2, layers=c(5,5,3), activation=c('sigmoid','sigmoid','softmax'))
X.trn = rbind( mvrnorm(75, mu=c(2,0), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(75, mu=c(6,4), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(75, mu=c(2,6), Sigma = diag(.25,nrow=2,ncol=2)))
Y.trn = matrix( c(rep(c(1,0,0), 75),
rep(c(0,1,0), 75),
rep(c(0,0,1), 75)), ncol=3, byrow=TRUE )
nn.trn = train(nn,X.trn,Y.trn, epochs=1000, mini.batch.size=15, learning.rate=0.1)
nn.prd = predict(nn.trn, X.trn)
Y.trn - nn.prd
nn = NNModel(input.dim = 2, layers=c(5,5,1), activation=c('sigmoid','sigmoid','softmax'))
X.trn = rbind( mvrnorm(75, mu=c(2,0), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(75, mu=c(6,4), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(75, mu=c(2,6), Sigma = diag(.25,nrow=2,ncol=2)))
Y.trn = matrix( c(rep(c(1,0,0), 75),
rep(c(0,1,0), 75),
rep(c(0,0,1), 75)), ncol=3, byrow=TRUE )
X.trn = rbind( mvrnorm(50, mu=c(1,2), Sigma = diag(1,nrow=2,ncol=2)),
mvrnorm(50, mu=c(1.5,2.5), Sigma = diag(5,nrow=2,ncol=2)))
Y.trn = matrix( c(rep(0,50), rep(1,50)) )
nn.trn = train(nn,X.trn,Y.trn, epochs=1000, mini.batch.size=15, learning.rate=0.1)

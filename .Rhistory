z
NNModel = function( layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# layers = vector of number of nodes per layers - first entry is dimension  of input
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( paste('L', 1:length(layers)), {
'blah'
})
}
z = NNModel()
nn$layers = lapply( paste('L', 1:length(layers)), function() {
'blah'
})
NNModel = function( layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# layers = vector of number of nodes per layers - first entry is dimension  of input
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( paste('L', 1:length(layers)), function() {
'blah'
})
}
z = NNModel()
NNModel = function( layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# layers = vector of number of nodes per layers - first entry is dimension  of input
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( paste('L', 1:length(layers)), function(l) {
'blah'
})
}
z = NNModel()
z
layers = c(1,2,3,4)
paste('L', 1:length(layers))
paste('L', 1:length(layers), sep='')
NNModel = function( layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# layers = vector of number of nodes per layers - first entry is dimension  of input
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( paste('L', 1:length(layers), sep=''), function(l) {
'blah'
})
nn
}
z = NNModel()
z
names(z)
names(z$layers)
2:(length(layers)+1)
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 2:(length(layers)+1), function(l) {
'blah'
})
names(nn$layers) = paste('L', 1:length(layers), sep='')
nn
}
z = NNModel()
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 2:(length(layers)+1), function(l) {
'blah'
})
names(nn$layers) = paste('L', 2:(length(layers)+1), sep='')
nn
}
z = NNModel()
z
2:(length(layers)+1)
paste('L', 2:(length(layers)+1), sep='')
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 2:(length(layers)+1), function(l) {
activation = activations[l]
if( l == 2){
weights = matrix(rnorm( length(input.dim)*layers[l], 0, 0.01),
nrow = length(input.dim) )
}
})
names(nn$layers) = paste('L', 2:(length(layers)+1), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
print()
nn$layers = lapply( 2:(length(layers)+1), function(l) {
activation = activations[l]
if( l == 2){
weights = matrix(rnorm( input.dim*layers[l], 0, 0.01),
nrow = length(input.dim) )
}
})
names(nn$layers) = paste('L', 2:(length(layers)+1), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 2:(length(layers)+1), function(l) {
activation = activations[l]
if( l == 2){
weights = matrix(rnorm( input.dim*layers[l], 0, 0.01),
nrow = length(input.dim) )
}
})
names(nn$layers) = paste('L', 2:(length(layers)+1), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 2:(length(layers)+1), function(l) {
activation = activations[l]
if( l == 2){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = length(input.dim) )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
})
names(nn$layers) = paste('L', 2:(length(layers)+1), sep='')
nn
}
z
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
activation = activations[l]
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = length(input.dim) )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
})
names(nn$layers) = paste('L', 2:(length(layers)+1), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
activation = activations[l]
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = length(input.dim) )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
})
names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
activation = activations[l]
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = input.di) )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
})
names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
activation = activations[l]
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = input.dim )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
})
names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
activation = activations[l]
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = input.dim )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
})
#names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = input.dim )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
c( activation = activations[l],
weights = weights)
})
#names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = input.dim )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
list( activation = activations[l],
weights = weights)
})
#names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = input.dim )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
list( activation = activations[l],
weights = weights)
})
names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
z = NNModel(input.dim = 100, layers = c(50,10,10,1), activations = c('sigmoid', 'sigmoid', sigmoid', 'softmax'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z = NNModel(input.dim = 100, layers = c(50,10,10,1), activations = c('sigmoid', 'sigmoid', 'sigmoid', 'softmax'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
x=rep(1,10)
x
a = forward.prop(NNModel, x)
a = forward.prop(z, x)
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
a = forward.prop(z, x)
x=matrix(10,x,nrow=10)
x
a = forward.prop(z, x)
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
a = forward.prop(z, x)
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
a = forward.prop(z, x)
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
a = forward.prop(z, x)
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
a = forward.prop(z, x)
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
z
x
a
z$layers$L1$weights[1,]
z$layers$L1$weights[1,]*10
sum(z$layers$L1$weights[1,]*10)
sigmoid(-0.2845585)
sum(z$layers$L1$weights[,1]*10)
z$layers$L1$weights[,1]
z$layers$L1$weights[,1]*10
sum(z$layers$L1$weights[,1]*10)
sigmoid(sum(z$layers$L1$weights[,1]*10))
z = NNModel(input.dim = 10, layers = c(1), activations = c('linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
a = forward.prop(z, x)
a
x
x=matrix(x,nrow=10)
x
x=matrix(rep(1,10),nrow=10)
x
a = forward.prop(z, x)
a
z
z
a
sum(z$layers$L1$weights)
z = NNModel(input.dim = 10, layers = c(1), activations = c('sigmoid'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
a = forward.prop(z, x)
a
tanh(sum(z$layers$L1$weights))
a
z
a
x
tanh(sum(z$layers$L1$weights))
a
z = NNModel(input.dim = 10, layers = c(1), activations = c('tanh'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
a = forward.prop(z, x)
a
tanh(sum(z$layers$L1$weights))
z = NNModel(input.dim = 10, layers = c(1), activations = c('relu'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
a = forward.prop(z, x)
a
(sum(z$layers$L1$weights))
z = NNModel(input.dim = 10, layers = c(1), activations = c('relu'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
(sum(z$layers$L1$weights))
z = NNModel(input.dim = 10, layers = c(1), activations = c('relu'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
(sum(z$layers$L1$weights))
a
a = forward.prop(z, x)
a
setwd("/storage/ColoComp/Development/NNFS")
source('/storage/ColoComp/Development/NNFS/NNFS.R')
nn = NNModel(input.dim=2, layers=1, activations='linear', learning.rate=0.1)
X.trn = matrix(rnorm(200,0,1), nrow=100)
Y.trn = matrix(5 * X.trn[,1] - 0.25 * X.trn[, 2], nrow=100)
nn.trn = train(nn,X.trn,Y.trn, epochs=5, mini.batch.size=15)
nn.trn
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
nn.trn
Y.trn = matrix(5 * X.trn[,1] - 7.25 * X.trn[, 2], nrow=100)
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
nn.trn
X.trn
X.trn = matrix(rnorm(300,0,1), nrow=100)
Y.trn = matrix(5 * X.trn[,1] - 7.25 * X.trn[, 2] + 6.83 * X.trn[, 3], nrow=100)
Y.trn
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
traceback
traceback()
nn = NNModel(input.dim=3, layers=1, activations='linear', learning.rate=0.1)
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
nn.trn
nn.trn = train(nn,X.trn,Y.trn, epochs=25, mini.batch.size=15)
nn.trn
nn.trn = train(nn,X.trn,Y.trn, epochs=100, mini.batch.size=15)
nn.trn
nn = NNModel(input.dim=3, layers=1, activations='linear', learning.rate=0.5)
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
nn.trn
source('/storage/ColoComp/Development/NNFS/NNFS.R')

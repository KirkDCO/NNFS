# layers = vector of number of nodes per layers - first entry is dimension  of input
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( paste('L', 1:length(layers), sep=''), function(l) {
'blah'
})
nn
}
z = NNModel()
z
names(z)
names(z$layers)
2:(length(layers)+1)
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 2:(length(layers)+1), function(l) {
'blah'
})
names(nn$layers) = paste('L', 1:length(layers), sep='')
nn
}
z = NNModel()
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 2:(length(layers)+1), function(l) {
'blah'
})
names(nn$layers) = paste('L', 2:(length(layers)+1), sep='')
nn
}
z = NNModel()
z
2:(length(layers)+1)
paste('L', 2:(length(layers)+1), sep='')
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 2:(length(layers)+1), function(l) {
activation = activations[l]
if( l == 2){
weights = matrix(rnorm( length(input.dim)*layers[l], 0, 0.01),
nrow = length(input.dim) )
}
})
names(nn$layers) = paste('L', 2:(length(layers)+1), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
print()
nn$layers = lapply( 2:(length(layers)+1), function(l) {
activation = activations[l]
if( l == 2){
weights = matrix(rnorm( input.dim*layers[l], 0, 0.01),
nrow = length(input.dim) )
}
})
names(nn$layers) = paste('L', 2:(length(layers)+1), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 2:(length(layers)+1), function(l) {
activation = activations[l]
if( l == 2){
weights = matrix(rnorm( input.dim*layers[l], 0, 0.01),
nrow = length(input.dim) )
}
})
names(nn$layers) = paste('L', 2:(length(layers)+1), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 2:(length(layers)+1), function(l) {
activation = activations[l]
if( l == 2){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = length(input.dim) )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
})
names(nn$layers) = paste('L', 2:(length(layers)+1), sep='')
nn
}
z
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
activation = activations[l]
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = length(input.dim) )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
})
names(nn$layers) = paste('L', 2:(length(layers)+1), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
activation = activations[l]
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = length(input.dim) )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
})
names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
activation = activations[l]
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = input.di) )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
})
names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
activation = activations[l]
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = input.dim )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
})
names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
activation = activations[l]
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = input.dim )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
})
#names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = input.dim )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
c( activation = activations[l],
weights = weights)
})
#names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = input.dim )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
list( activation = activations[l],
weights = weights)
})
#names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = input.dim )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
list( activation = activations[l],
weights = weights)
})
names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
z = NNModel(input.dim = 100, layers = c(50,10,10,1), activations = c('sigmoid', 'sigmoid', sigmoid', 'softmax'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z = NNModel(input.dim = 100, layers = c(50,10,10,1), activations = c('sigmoid', 'sigmoid', 'sigmoid', 'softmax'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
x=rep(1,10)
x
a = forward.prop(NNModel, x)
a = forward.prop(z, x)
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
a = forward.prop(z, x)
x=matrix(10,x,nrow=10)
x
a = forward.prop(z, x)
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
a = forward.prop(z, x)
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
a = forward.prop(z, x)
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
a = forward.prop(z, x)
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
a = forward.prop(z, x)
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
z
x
a
z$layers$L1$weights[1,]
z$layers$L1$weights[1,]*10
sum(z$layers$L1$weights[1,]*10)
sigmoid(-0.2845585)
sum(z$layers$L1$weights[,1]*10)
z$layers$L1$weights[,1]
z$layers$L1$weights[,1]*10
sum(z$layers$L1$weights[,1]*10)
sigmoid(sum(z$layers$L1$weights[,1]*10))
z = NNModel(input.dim = 10, layers = c(1), activations = c('linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
a = forward.prop(z, x)
a
x
x=matrix(x,nrow=10)
x
x=matrix(rep(1,10),nrow=10)
x
a = forward.prop(z, x)
a
z
z
a
sum(z$layers$L1$weights)
z = NNModel(input.dim = 10, layers = c(1), activations = c('sigmoid'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
a = forward.prop(z, x)
a
tanh(sum(z$layers$L1$weights))
a
z
a
x
tanh(sum(z$layers$L1$weights))
a
z = NNModel(input.dim = 10, layers = c(1), activations = c('tanh'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
a = forward.prop(z, x)
a
tanh(sum(z$layers$L1$weights))
z = NNModel(input.dim = 10, layers = c(1), activations = c('relu'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
a = forward.prop(z, x)
a
(sum(z$layers$L1$weights))
z = NNModel(input.dim = 10, layers = c(1), activations = c('relu'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
(sum(z$layers$L1$weights))
z = NNModel(input.dim = 10, layers = c(1), activations = c('relu'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
(sum(z$layers$L1$weights))
a
a = forward.prop(z, x)
a
setwd("/storage/ColoComp/Development/NNFS")
sample(100)
source('/storage/ColoComp/Development/NNFS/NNFS.R')
nn = NNModel(2,c(1),'linear')
nn
train(nn, X=matrix(rnorm(200), nrows=100), mini.batch.size=10, epochs=1)
train(nn, X=matrix(rnorm(200), rows=100), mini.batch.size=10, epochs=1)
rnorm(200)
train(nn, X=matrix(rnorm(200), rows=100), mini.batch.size=10, epochs=1)
train(nn, X=matrix(rnorm(200), nrow=100), mini.batch.size=10, epochs=1)
source('/storage/ColoComp/Development/NNFS/NNFS.R')
train(nn, X=matrix(rnorm(200), nrow=100), mini.batch.size=10, epochs=1)
batches = ceiling(dim(X)[1]/mini.batch.size)
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
train(nn, X=matrix(rnorm(200), nrow=100), mini.batch.size=10, epochs=1)
batches
mini.batch.order
mb.start
mb
mb.start
mb.stop
source('/storage/ColoComp/Development/NNFS/NNFS.R')
train(nn, X=matrix(rnorm(200), nrow=100), mini.batch.size=10, epochs=1)
source('/storage/ColoComp/Development/NNFS/NNFS.R')
train(nn, X=matrix(rnorm(200), nrow=100), mini.batch.size=10, epochs=1)
train(nn, X=matrix(rnorm(200), nrow=100), mini.batch.size=15, epochs=1)
source('/storage/ColoComp/Development/NNFS/NNFS.R')
source('/storage/ColoComp/Development/NNFS/NNFS.R')
train(nn, X=matrix(rnorm(200), nrow=100), mini.batch.size=15, epochs=1)
source('/storage/ColoComp/Development/NNFS/NNFS.R')
train(nn, X=matrix(rnorm(200), nrow=100), mini.batch.size=15, epochs=1)
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
for( e in 1:epochs ){
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
train(nn, X=matrix(rnorm(200), nrow=100), mini.batch.size=15, epochs=1)
NNmod
mini.batch
layers
lout
source('/storage/ColoComp/Development/NNFS/NNFS.R')
source('/storage/ColoComp/Development/NNFS/NNFS.R')
source('/storage/ColoComp/Development/NNFS/NNFS.R')
train(nn, X=matrix(rnorm(200), nrow=100), mini.batch.size=15, epochs=1)
nn
X=matrix(rnorm(200)
)
X
trn.nn = train(nn, X, mini.batch.size=15, epochs=1)
source('/storage/ColoComp/Development/NNFS/NNFS.R')
nn = NNModel(2,c(1),'linear')
trn.nn = train(nn, X, mini.batch.size=15, epochs=1)
X
X=matrix(rnorm(200), nrow=100)
trn.nn = train(nn, X, mini.batch.size=15, epochs=1)
trn.nn
X
nn
X[100, ] %*% nn$layers$L1$weights
source('/storage/ColoComp/Development/NNFS/NNFS.R')
source('/storage/ColoComp/Development/NNFS/NNFS.R')
source('/storage/ColoComp/Development/NNFS/NNFS.R')
trn.nn = train(nn, X, mini.batch.size=15, epochs=1)
trn.nn
source('/storage/ColoComp/Development/NNFS/NNFS.R')
trn.nn = train(nn, X, mini.batch.size=15, epochs=1)
trn.nn
source('/storage/ColoComp/Development/NNFS/NNFS.R')
X
trn.nn
nn
source('/storage/ColoComp/Development/NNFS/NNFS.R')
trn.nn = train(nn, X, mini.batch.size=15, epochs=1)
trn.nn
source('/storage/ColoComp/Development/NNFS/NNFS.R')
trn.nn
nn
NNmod = forward.prop(NNmod, X=mini.batch)
trn.nn = train(nn, X, mini.batch.size=15, epochs=1)
NNmod
NNmod
100/15
15*6

weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = length(input.dim) )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
})
names(nn$layers) = paste('L', 2:(length(layers)+1), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
activation = activations[l]
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = length(input.dim) )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
})
names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
activation = activations[l]
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = input.di) )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
})
names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
activation = activations[l]
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = input.dim )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
})
names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
activation = activations[l]
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = input.dim )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
})
#names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = input.dim )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
c( activation = activations[l],
weights = weights)
})
#names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = input.dim )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
list( activation = activations[l],
weights = weights)
})
#names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = input.dim )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
list( activation = activations[l],
weights = weights)
})
names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
z = NNModel(input.dim = 100, layers = c(50,10,10,1), activations = c('sigmoid', 'sigmoid', sigmoid', 'softmax'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z = NNModel(input.dim = 100, layers = c(50,10,10,1), activations = c('sigmoid', 'sigmoid', 'sigmoid', 'softmax'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
x=rep(1,10)
x
a = forward.prop(NNModel, x)
a = forward.prop(z, x)
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
a = forward.prop(z, x)
x=matrix(10,x,nrow=10)
x
a = forward.prop(z, x)
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
a = forward.prop(z, x)
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
a = forward.prop(z, x)
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
a = forward.prop(z, x)
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
a = forward.prop(z, x)
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
z
x
a
z$layers$L1$weights[1,]
z$layers$L1$weights[1,]*10
sum(z$layers$L1$weights[1,]*10)
sigmoid(-0.2845585)
sum(z$layers$L1$weights[,1]*10)
z$layers$L1$weights[,1]
z$layers$L1$weights[,1]*10
sum(z$layers$L1$weights[,1]*10)
sigmoid(sum(z$layers$L1$weights[,1]*10))
z = NNModel(input.dim = 10, layers = c(1), activations = c('linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
a = forward.prop(z, x)
a
x
x=matrix(x,nrow=10)
x
x=matrix(rep(1,10),nrow=10)
x
a = forward.prop(z, x)
a
z
z
a
sum(z$layers$L1$weights)
z = NNModel(input.dim = 10, layers = c(1), activations = c('sigmoid'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
a = forward.prop(z, x)
a
tanh(sum(z$layers$L1$weights))
a
z
a
x
tanh(sum(z$layers$L1$weights))
a
z = NNModel(input.dim = 10, layers = c(1), activations = c('tanh'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
a = forward.prop(z, x)
a
tanh(sum(z$layers$L1$weights))
z = NNModel(input.dim = 10, layers = c(1), activations = c('relu'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
a = forward.prop(z, x)
a
(sum(z$layers$L1$weights))
z = NNModel(input.dim = 10, layers = c(1), activations = c('relu'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
(sum(z$layers$L1$weights))
z = NNModel(input.dim = 10, layers = c(1), activations = c('relu'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
(sum(z$layers$L1$weights))
a
a = forward.prop(z, x)
a
setwd("/storage/ColoComp/Development/NNFS")
source('/storage/ColoComp/Development/NNFS/NNFS.R')
source('/storage/ColoComp/Development/NNFS/NNFS.R')
source('/storage/ColoComp/Development/NNFS/NNFS.R')
source('/storage/ColoComp/Development/NNFS/NNFS.R')
source('/storage/ColoComp/Development/NNFS/NNFS.R')
source('/storage/ColoComp/Development/NNFS/NNFS.R')
nn = NNModel(input.dim=2, layers=1, activations='linear', learning.rate=0.1)
nn
X.trn = matrix(rnorm(200,0,0), nrow=100)
X.trn
X.trn = matrix(rnorm(200,0,1), nrow=100)
X.trn
nn.fp = forward.prop(nn, X.trn)
nn.fp
X.trn
Y.trn = 5 * X.trn[,1] - 0.25 * X.trn[, 2]
Y.trn
delta = Y.trn - NNmod$layers[[layer]]$a
NNmod = nn
delta = Y.trn - NNmod$layers[[layer]]$a
delta = Y.trn - NNmod$layers[['L1']]$a
Y.trn
NNmod$layers[['L1']]$a
NNmod
NNmod = nn.fp
delta = Y.trn - NNmod$layers[['L1']]$a
delta
delta * X.trn
delta %*% X.trn
delta
class(delta)
grad = apply(X.trn, 2, *)
grad = apply(X.trn, 2, function(Xi) { Xi * delta })
grad
delta * X.trn[,1]
NNmod
grad
NNmod$layers[[layer]]$weights
layer = 1
layer = 'L1'
NNmod$layers[[layer]]$weights
NNmod$learning.rate * avg.grad
avg.grad = colMeans(grad)
NNmod$learning.rate * avg.grad
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
NN.trn = train(nn,X.trn,Y.trn)
NN.trn = train(nn,X.trn,Y.trn, epochs=10)
NN.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
dim(Y.trn)
Y.trn
class(Y.trn)
Y.trn = matrix(5 * X.trn[,1] - 0.25 * X.trn[, 2], nrow=100)
Y.trn
NN.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
layers
NNmod$layers[[layer]]$weights
NNmod$learning.rate
avg.grad
class(avg.grad)
avg.grad = matrix(colMeans(grad), nrow = dim(grad)[2])
avg.grad
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
NN.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
NN.trn
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
NN.trn
NN.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
NN.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
NNmod$layers[[layer]]$weights
NNmod$layers[[layer]]$weights
NNmod$layers[[layer]]$weights
NNmod$layers[[layer]]$weights
nn
NNmod = nn
mini.batch.size = 15
epochs=10
seed = 20180813
#set the random seed here
set.seed(seed)
#compute number of batches needed for a full epoch
batches = ceiling(dim(X.trn)[1]/mini.batch.size)
e=1
mini.batch.order = sample(dim(X.trn)[1])
mb = 0
mb.start = (mini.batch.size*mb+1)
mb.stop = min((mini.batch.size*(mb+1)), dim(X.trn)[1])
mini.batch.X = X.trn[ mini.batch.order[mb.start:mb.stop], ]
mini.batch.Y = Y.trn[ mini.batch.order[mb.start:mb.stop], , drop=F]
NNmod = forward.prop(NNmod, X=mini.batch.X)
NNmod
X=mini.batch.X
layers = names(NNmod$layers)
layers
1
l = 1
layer = layers[l]
#get the right derivative function
if( NNmod$layers[[layer]]$activation == 'linear' ){
d = d.linear()
}else if( NNmod$layers[[layer]]$activation == 'sigmoid' ){
d = d.sigmoid(NULL)
}else if( NNmod$layers[[layer]]$activation == 'relu' ){
d = d.relu(NULL)
}else if(NNmod$layers[[layer]]$activation == 'tanh' ){
d = d.tanh(NULL)
}else if(NNmod$layers[[layer]]$activation == 'softmax' ){
d = d.softmax(NULL)
}
d
delta = Y.trn - NNmod$layers[[layer]]$a
Y.trn
NNmod$layers[[layer]]$a
delta = mini.batch.Y - NNmod$layers[[layer]]$a
delta
grad = apply(mini.batch.X, 2, function(Xi) { Xi * delta })
grad
avg.grad = matrix(colMeans(grad), nrow = dim(grad)[2])
avg.grad
NNmod$layers[[layer]]$weights = NNmod$layers[[layer]]$weights -
NNmod$learning.rate * avg.grad
NNmod
source('/storage/ColoComp/Development/NNFS/NNFS.R')
nn = NNModel(input.dim=2, layers=1, activations='linear', learning.rate=0.1)
source('/storage/ColoComp/Development/NNFS/NNFS.R')
nn = NNModel(input.dim=2, layers=1, activations='linear', learning.rate=0.1)
X.trn = matrix(rnorm(200,0,1), nrow=100)
Y.trn = matrix(5 * X.trn[,1] - 0.25 * X.trn[, 2], nrow=100)
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
traceback()
source('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
nn.trn
nn.trn = train(nn,X.trn,Y.trn, epochs=100, mini.batch.size=15)
nn.trn
nn$learning.rate = 0.01
nn.trn = train(nn,X.trn,Y.trn, epochs=100, mini.batch.size=15)
nn.trn
source('/storage/ColoComp/Development/NNFS/NNFS.R')
source('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn
nn.trn = train(nn,X.trn,Y.trn, epochs=100, mini.batch.size=15)
nn.trn = train(nn,X.trn,Y.trn, epochs=5, mini.batch.size=15)
nn.trn = train(nn,X.trn,Y.trn, epochs=5, mini.batch.size=15)
nn.fp
nn
nn.fp = forward.prop(nn, X.trn)
nn.fp
delta = Y.trn - nn$layers[1]$a
delta
Y.trn
nn$layers[1]$a
delta = Y.trn - nn.fp$layers[1]$a
delta
nn.fp$layers[1]$a
nn.fp$layers[1]
nn.fp$layers[1]$a
nn.fp$layers[1]
delta = Y.trn - nn.fp$layers[['L1']]$a
delta
Y.trn
X.trn
plot(X.trn[,1], Y.trn)
delta
avg.grad
nn
source('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=5, mini.batch.size=15)
nn.trn
grad
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn
nn.trn = train(nn,X.trn,Y.trn, epochs=5, mini.batch.size=15)
grad
NNmod$layers[[layer]]$weights
t(NNmod$layers[[layer]]$weights)
source('/storage/ColoComp/Development/NNFS/NNFS.R')
source('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=5, mini.batch.size=15)
traceback()
source('/storage/ColoComp/Development/NNFS/NNFS.R')
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=5, mini.batch.size=15)
grad
dim(grad)
grad = apply(grad, 2, function(r) {r * t( NNmod$layers[[layer]]$weights)})
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=5, mini.batch.size=15)
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=5, mini.batch.size=15)
grad = apply(X.trn, 2, function(Xi) { Xi * delta })
grad
source('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=5, mini.batch.size=15)
traceback()
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=5, mini.batch.size=15)
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=5, mini.batch.size=15)
apply(X.trn, 2, function(Xi) { Xi * delta })
NNmod$layers[[layer]]$weights)
NNmod$layers[[layer]]$weight
t( NNmod$layers[[layer]]$weights)
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
t(NNmod$layers[[layer]]$weights)
NNmod = nn.trn
t(NNmod$layers[[layer]]$weights)
t(NNmod$layers[['L1']]$weights)
z=t(NNmod$layers[['L1']]$weights)
z
rep(z,15)
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=5, mini.batch.size=15)
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=5, mini.batch.size=15)
grad = apply(X.trn, 2, function(Xi) { Xi * delta })
grad
multi = matrix(rep( t(NNmod$layers[[layer]]$weights), dim(grad)[1]), nrow=dim(grad)[2])
multi
multi = matrix(rep( t(NNmod$layers[[layer]]$weights), dim(grad)[1]), nrow=dim(grad)[1])
multi
multi = matrix(rep( (NNmod$layers[[layer]]$weights), dim(grad)[1]), nrow=dim(grad)[1])
multi
matrix(rep( t(NNmod$layers[[layer]]$weights), dim(grad)[1]), ncol=dim(grad)[2])
multi = matrix(rep((NNmod$layers[[layer]]$weights), dim(grad)[1]), ncol=dim(grad)[2])
multi
rep((NNmod$layers[[layer]]$weights), dim(grad)[1])
multi = matrix(rep((NNmod$layers[[layer]]$weights), dim(grad)[1]), nrow=dim(grad)[1], byrow=T)
multi
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=5, mini.batch.size=15)
grad
avg.grad
source('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=5, mini.batch.size=15)
nn.trn
source('/storage/ColoComp/Development/NNFS/NNFS.R')

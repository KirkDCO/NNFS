mvrnorm(75, mu=c(2,6), Sigma = diag(.25,nrow=2,ncol=2)))
Y.trn = matrix( c(rep(0,75), rep(1,200), rep(2,75)) )
#plot classes and decision boundary
plot(X.trn[,1], X.trn[,2], col=c(rep('red',75),rep('blue',75),rep('green',75)),
pch=19)
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
# 3-layer multinomial, 3 clusters
######################################################
library(MASS)
nn = NNModel(input.dim = 2, layers=c(5,5,1), activation=c('sigmoid','sigmoid','softmax'))
X.trn = rbind( mvrnorm(75, mu=c(2,0), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(75, mu=c(6,4), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(75, mu=c(2,6), Sigma = diag(.25,nrow=2,ncol=2)))
Y.trn = matrix( c(rep(0,75), rep(1,200), rep(2,75)) )
nn.trn = train(nn,X.trn,Y.trn, epochs=50000, mini.batch.size=15, learning.rate=0.1)
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=50000, mini.batch.size=15, learning.rate=0.1)
delta
Y.trn
NNmod.old$layers[[layer]]$z
NNmod.old$layers[[layer+1]]$z
layer
NNmod
NNmod.old$layers[['L3']]$z
NNmod.old$layers[['L3']]$a
error(Y.trn, NNmod.old$layers[[layer]]$z)
error(Y.trn, NNmod.old$layers[[L3]]$z)
error(Y.trn, NNmod.old$layers[['L3']]$z)
Y.trn
delta
Y.trn = matrix( rep(c(1,0,0), 75) )
Y.trn
Y.trn = matrix( rep(c(1,0,0), 75), ncol=3 )
Y.trn
Y.trn = matrix( rep(c(1,0,0), 75), ncol=3, byrow=TRUE )
Y.trn
rep(c(0,1,0), 75),
rep(c(0,0,1), 75), ncol=3, byrow=TRUE )
Y.trn = matrix( c(rep(c(1,0,0), 75)
rep(c(0,1,0), 75),
rep(c(0,0,1), 75)), ncol=3, byrow=TRUE )
Y.trn = matrix( c(rep(c(1,0,0), 75)
rep(c(0,1,0), 75),
rep(c(0,0,1), 75)), ncol=3, byrow=TRUE )
Y.trn = matrix( c(rep(c(1,0,0), 75),
rep(c(0,1,0), 75),
rep(c(0,0,1), 75)), ncol=3, byrow=TRUE )
Ytrn
Y.;trn
Y.trn
#plot classes and decision boundary
plot(X.trn[,1], X.trn[,2], col=c(rep('red',75),rep('blue',75),rep('green',75)),
pch=19)
X.trn = rbind( mvrnorm(75, mu=c(2,0), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(75, mu=c(6,4), Sigma = diag(.25,nrow=2,ncol=2)),
mvrnorm(75, mu=c(2,6), Sigma = diag(.25,nrow=2,ncol=2)))
Y.trn = matrix( c(rep(c(1,0,0), 75),
rep(c(0,1,0), 75),
rep(c(0,0,1), 75)), ncol=3, byrow=TRUE )
#plot classes and decision boundary
plot(X.trn[,1], X.trn[,2], col=c(rep('red',75),rep('blue',75),rep('green',75)),
pch=19)
X.trn = matrix(c(rnorm(50,mean=2,sd=.5), rnorm(50,mean=4,sd=.5)), nrow=100)
Y.trn = matrix(c(rep(0,50), rep(1,50)))
plot(X.trn, Y.trn)
bars(1:10,col=1:10)
barplot(1:10,col=1:10)
?graphical.parameters
?grahical
??`graphical parameter`
pie(rep(1, 12), col = rainbow(12))
pie(rep(1, 10), col = rainbow(10))
demo.pal <-
function(n, border = if (n < 32) "light gray" else NA,
main = paste("color palettes;  n=", n),
ch.col = c("rainbow(n, start=.7, end=.1)", "heat.colors(n)",
"terrain.colors(n)", "topo.colors(n)",
"cm.colors(n)"))
{
nt <- length(ch.col)
i <- 1:n; j <- n / nt; d <- j/6; dy <- 2*d
plot(i, i+d, type = "n", yaxt = "n", ylab = "", main = main)
for (k in 1:nt) {
rect(i-.5, (k-1)*j+ dy, i+.4, k*j,
col = eval(parse(text = ch.col[k])), border = border)
text(2*j,  k * j + dy/4, ch.col[k])
}
}
n <- if(.Device == "postscript") 64 else 16
# Since for screen, larger n may give color allocation problem
demo.pal(n)
pie(rep(1, 20), col = rainbow(20))
source('/storage/ColoComp/Development/NNFS/NNFS.R')
# 3-layer multinomial, 10 randomly placed clusters
##################################################
library(MASS)
nn = NNModel(input.dim = 2, layers=c(5,5,10), activation=c('sigmoid','sigmoid','softmax'),seed=180930)
x1.rnd = runif(10,-1,1)
x2.rnd = runif(10,-1,1)
s.rnd = rnorm(n=10,mean=0.1,sd=.05)
X.trn = rbind( mvrnorm(100, mu=c(x1.rnd[1], x2.rnd[1]), Sigma = diag(s.rnd[1],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[2], x2.rnd[2]), Sigma = diag(s.rnd[2],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[3], x2.rnd[3]), Sigma = diag(s.rnd[3],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[4], x2.rnd[4]), Sigma = diag(s.rnd[4],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[5], x2.rnd[5]), Sigma = diag(s.rnd[5],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[6], x2.rnd[6]), Sigma = diag(s.rnd[6],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[7], x2.rnd[7]), Sigma = diag(s.rnd[7],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[8], x2.rnd[8]), Sigma = diag(s.rnd[8],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[9], x2.rnd[9]), Sigma = diag(s.rnd[9],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[10], x2.rnd[10]), Sigma = diag(s.rnd[10],nrow=2,ncol=2)))
Y.trn = matrix( c(rep(c(1,0,0,0,0,0,0,0,0,0), 100),
rep(c(0,1,0,0,0,0,0,0,0,0), 100),
rep(c(0,0,1,0,0,0,0,0,0,0), 100),
rep(c(0,0,0,1,0,0,0,0,0,0), 100),
rep(c(0,0,0,0,1,0,0,0,0,0), 100),
rep(c(0,0,0,0,0,1,0,0,0,0), 100),
rep(c(0,0,0,0,0,0,1,0,0,0), 100),
rep(c(0,0,0,0,0,0,0,1,0,0), 100),
rep(c(0,0,0,0,0,0,0,0,1,0), 100),
rep(c(0,0,0,0,0,0,0,0,0,1), 100)), ncol=10, byrow=TRUE )
nn.trn = train(nn,X.trn,Y.trn, epochs=500, mini.batch.size=250, learning.rate=0.1)
#plot classes and decision boundary
plot(X.trn[,1], X.trn[,2], col=rep(rainbow(10),each=100), pch=19)
for( x1 in seq(from=min(X.trn[,1]), to=max(X.trn[,1]), length.out=200) ) {
for( x2 in seq(from=min(X.trn[,2]), to=max(X.trn[,2]), length.out=200) ){
prd = predict(nn.trn, matrix(c(x1,x2), nrow=1))
clr = rainbow(10)[ which.max(prd) ]
points(x1,x2,pch=19,cex=.15,col=clr)
}
}
#plot classes and decision boundary
plot(X.trn[,1], X.trn[,2], col=rep(rainbow(10),each=100), pch=19)
source('/storage/ColoComp/Development/NNFS/NNFS.R')
# 3-layer multinomial, 10 randomly placed clusters
##################################################
library(MASS)
nn = NNModel(input.dim = 2, layers=c(5,5,10), activation=c('sigmoid','sigmoid','softmax'),seed=180930)
x1.rnd = runif(10,-1,1)
x2.rnd = runif(10,-1,1)
s.rnd = rnorm(n=10,mean=0.1,sd=.05)
X.trn = rbind( mvrnorm(100, mu=c(x1.rnd[1], x2.rnd[1]), Sigma = diag(s.rnd[1],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[2], x2.rnd[2]), Sigma = diag(s.rnd[2],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[3], x2.rnd[3]), Sigma = diag(s.rnd[3],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[4], x2.rnd[4]), Sigma = diag(s.rnd[4],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[5], x2.rnd[5]), Sigma = diag(s.rnd[5],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[6], x2.rnd[6]), Sigma = diag(s.rnd[6],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[7], x2.rnd[7]), Sigma = diag(s.rnd[7],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[8], x2.rnd[8]), Sigma = diag(s.rnd[8],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[9], x2.rnd[9]), Sigma = diag(s.rnd[9],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[10], x2.rnd[10]), Sigma = diag(s.rnd[10],nrow=2,ncol=2)))
Y.trn = matrix( c(rep(c(1,0,0,0,0,0,0,0,0,0), 100),
rep(c(0,1,0,0,0,0,0,0,0,0), 100),
rep(c(0,0,1,0,0,0,0,0,0,0), 100),
rep(c(0,0,0,1,0,0,0,0,0,0), 100),
rep(c(0,0,0,0,1,0,0,0,0,0), 100),
rep(c(0,0,0,0,0,1,0,0,0,0), 100),
rep(c(0,0,0,0,0,0,1,0,0,0), 100),
rep(c(0,0,0,0,0,0,0,1,0,0), 100),
rep(c(0,0,0,0,0,0,0,0,1,0), 100),
rep(c(0,0,0,0,0,0,0,0,0,1), 100)), ncol=10, byrow=TRUE )
#plot classes and decision boundary
plot(X.trn[,1], X.trn[,2], col=rep(rainbow(10),each=100), pch=19)
nn.trn = train(nn,X.trn,Y.trn, epochs=500, mini.batch.size=250, learning.rate=0.1)
for( x1 in seq(from=min(X.trn[,1]), to=max(X.trn[,1]), length.out=200) ) {
for( x2 in seq(from=min(X.trn[,2]), to=max(X.trn[,2]), length.out=200) ){
prd = predict(nn.trn, matrix(c(x1,x2), nrow=1))
clr = rainbow(10)[ which.max(prd) ]
points(x1,x2,pch=19,cex=.15,col=clr)
}
}
nn.trn = train(nn,X.trn,Y.trn, epochs=1000, mini.batch.size=250, learning.rate=0.1)
#plot classes and decision boundary
plot(X.trn[,1], X.trn[,2], col=rep(rainbow(10),each=100), pch=19)
for( x1 in seq(from=min(X.trn[,1]), to=max(X.trn[,1]), length.out=200) ) {
for( x2 in seq(from=min(X.trn[,2]), to=max(X.trn[,2]), length.out=200) ){
prd = predict(nn.trn, matrix(c(x1,x2), nrow=1))
clr = rainbow(10)[ which.max(prd) ]
points(x1,x2,pch=19,cex=.15,col=clr)
}
}
nn.trn = train(nn,X.trn,Y.trn, epochs=1000, mini.batch.size=25, learning.rate=0.1)
#plot classes and decision boundary
plot(X.trn[,1], X.trn[,2], col=rep(rainbow(10),each=100), pch=19)
for( x1 in seq(from=min(X.trn[,1]), to=max(X.trn[,1]), length.out=200) ) {
for( x2 in seq(from=min(X.trn[,2]), to=max(X.trn[,2]), length.out=200) ){
prd = predict(nn.trn, matrix(c(x1,x2), nrow=1))
clr = rainbow(10)[ which.max(prd) ]
points(x1,x2,pch=19,cex=.15,col=clr)
}
}
source('/storage/ColoComp/Development/NNFS/NNFS.R')
# 3-layer multinomial, 10 randomly placed clusters
##################################################
library(MASS)
nn = NNModel(input.dim = 2, layers=c(5,5,10), activation=c('sigmoid','sigmoid','softmax'),seed=180930)
x1.rnd = runif(10,-1,1)
x2.rnd = runif(10,-1,1)
s.rnd = rnorm(n=10,mean=0.1,sd=.05)
X.trn = rbind( mvrnorm(100, mu=c(x1.rnd[1], x2.rnd[1]), Sigma = diag(s.rnd[1],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[2], x2.rnd[2]), Sigma = diag(s.rnd[2],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[3], x2.rnd[3]), Sigma = diag(s.rnd[3],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[4], x2.rnd[4]), Sigma = diag(s.rnd[4],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[5], x2.rnd[5]), Sigma = diag(s.rnd[5],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[6], x2.rnd[6]), Sigma = diag(s.rnd[6],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[7], x2.rnd[7]), Sigma = diag(s.rnd[7],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[8], x2.rnd[8]), Sigma = diag(s.rnd[8],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[9], x2.rnd[9]), Sigma = diag(s.rnd[9],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[10], x2.rnd[10]), Sigma = diag(s.rnd[10],nrow=2,ncol=2)))
Y.trn = matrix( c(rep(c(1,0,0,0,0,0,0,0,0,0), 100),
rep(c(0,1,0,0,0,0,0,0,0,0), 100),
rep(c(0,0,1,0,0,0,0,0,0,0), 100),
rep(c(0,0,0,1,0,0,0,0,0,0), 100),
rep(c(0,0,0,0,1,0,0,0,0,0), 100),
rep(c(0,0,0,0,0,1,0,0,0,0), 100),
rep(c(0,0,0,0,0,0,1,0,0,0), 100),
rep(c(0,0,0,0,0,0,0,1,0,0), 100),
rep(c(0,0,0,0,0,0,0,0,1,0), 100),
rep(c(0,0,0,0,0,0,0,0,0,1), 100)), ncol=10, byrow=TRUE )
nn.trn = train(nn,X.trn,Y.trn, epochs=1000, mini.batch.size=25, learning.rate=0.1)
#plot classes and decision boundary
plot(X.trn[,1], X.trn[,2], col=rep(rainbow(10),each=100), pch=19)
for( x1 in seq(from=min(X.trn[,1]), to=max(X.trn[,1]), length.out=200) ) {
for( x2 in seq(from=min(X.trn[,2]), to=max(X.trn[,2]), length.out=200) ){
prd = predict(nn.trn, matrix(c(x1,x2), nrow=1))
clr = rainbow(10)[ which.max(prd) ]
points(x1,x2,pch=19,cex=.15,col=clr)
}
}
setwd("/storage/ColoComp/Development/NNFS")
source('/storage/ColoComp/Development/NNFS/NNFS.R')
# 3-layer multinomial, 6 clusters
#################################
library(MASS)
nn = NNModel(input.dim = 2, layers=c(5,5,6), activation=c('sigmoid','sigmoid','softmax'))
X.trn = rbind( mvrnorm(100, mu=c(0,.75), Sigma = diag(.1,nrow=2,ncol=2)),
mvrnorm(100, mu=c(.75,.5), Sigma = diag(.1,nrow=2,ncol=2)),
mvrnorm(100, mu=c(.75,-.5), Sigma = diag(.1,nrow=2,ncol=2)),
mvrnorm(100, mu=c(0,-.75), Sigma = diag(.1,nrow=2,ncol=2)),
mvrnorm(100, mu=c(-.75,-.5), Sigma = diag(.1,nrow=2,ncol=2)),
mvrnorm(100, mu=c(-.75,.5), Sigma = diag(.1,nrow=2,ncol=2)))
Y.trn = matrix( c(rep(c(1,0,0,0,0,0), 100),
rep(c(0,1,0,0,0,0), 100),
rep(c(0,0,1,0,0,0), 100),
rep(c(0,0,0,1,0,0), 100),
rep(c(0,0,0,0,1,0), 100),
rep(c(0,0,0,0,0,1), 100)), ncol=6, byrow=TRUE )
nn.trn = train(nn,X.trn,Y.trn, epochs=2500, mini.batch.size=15, learning.rate=0.1)
prd_old = predict(nn.trn, matrix(c(min(X.trn[,1]),min(X.trn[,2])))
for( x1 in seq(from=min(X.trn[,1]), to=max(X.trn[,1]), length.out=200) ) {
for( x2 in seq(from=min(X.trn[,2]), to=max(X.trn[,2]), length.out=200) ){
prd = predict(nn.trn, matrix(c(x1,x2), nrow=1))
clr = c('red', 'blue', 'green','purple','darkorange','cyan')[ which.max(prd) ]
points(x1,x2,pch=19,cex=.15,col=clr)
if( prd != prd_old ){
points(x1,x2-0.5,pch=19,cex=0.15,col='black')
}
prd_old = prd
}
}
}
# 3-layer multinomial, 10 randomly placed clusters
##################################################
library(MASS)
nn = NNModel(input.dim = 2, layers=c(5,5,10), activation=c('sigmoid','sigmoid','softmax'),seed=180930)
x1.rnd = runif(10,-1,1)
x2.rnd = runif(10,-1,1)
s.rnd = rnorm(n=10,mean=0.1,sd=.05)
X.trn = rbind( mvrnorm(100, mu=c(x1.rnd[1], x2.rnd[1]), Sigma = diag(s.rnd[1],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[2], x2.rnd[2]), Sigma = diag(s.rnd[2],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[3], x2.rnd[3]), Sigma = diag(s.rnd[3],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[4], x2.rnd[4]), Sigma = diag(s.rnd[4],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[5], x2.rnd[5]), Sigma = diag(s.rnd[5],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[6], x2.rnd[6]), Sigma = diag(s.rnd[6],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[7], x2.rnd[7]), Sigma = diag(s.rnd[7],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[8], x2.rnd[8]), Sigma = diag(s.rnd[8],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[9], x2.rnd[9]), Sigma = diag(s.rnd[9],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[10], x2.rnd[10]), Sigma = diag(s.rnd[10],nrow=2,ncol=2)))
Y.trn = matrix( c(rep(c(1,0,0,0,0,0,0,0,0,0), 100),
rep(c(0,1,0,0,0,0,0,0,0,0), 100),
rep(c(0,0,1,0,0,0,0,0,0,0), 100),
rep(c(0,0,0,1,0,0,0,0,0,0), 100),
rep(c(0,0,0,0,1,0,0,0,0,0), 100),
rep(c(0,0,0,0,0,1,0,0,0,0), 100),
rep(c(0,0,0,0,0,0,1,0,0,0), 100),
rep(c(0,0,0,0,0,0,0,1,0,0), 100),
rep(c(0,0,0,0,0,0,0,0,1,0), 100),
rep(c(0,0,0,0,0,0,0,0,0,1), 100)), ncol=10, byrow=TRUE )
nn.trn = train(nn,X.trn,Y.trn, epochs=1000, mini.batch.size=25, learning.rate=0.1)
nn.prd = predict(nn.trn, X.trn)
Y.trn - nn.prd
#plot classes and decision boundary
plot(X.trn[,1], X.trn[,2], col=rep(rainbow(10),each=100), pch=19)
for( x1 in seq(from=min(X.trn[,1]), to=max(X.trn[,1]), length.out=200) ) {
for( x2 in seq(from=min(X.trn[,2]), to=max(X.trn[,2]), length.out=200) ){
prd = predict(nn.trn, matrix(c(x1,x2), nrow=1))
clr = rainbow(10)[ which.max(prd) ]
points(x1,x2,pch=19,cex=.15,col=clr)
}
}
# Spiral problem
################
#data generation modified from:  https://stats.stackexchange.com/questions/164048/can-a-random-forest-be-used-for-feature-selection-in-multiple-linear-regression
#basic
n <- 1:2000
r <- 0.05*n +1
th <- n*(4*pi)/max(n)
#polar to cartesian
x1=r*cos(th)
y1=r*sin(th)
#add noise
x2 <- x1+0.1*r*runif(min = -1,max = 1,n=length(n))
y2 <- y1+0.1*r*runif(min = -1,max = 1,n=length(n))
#append salt and pepper
x3 <- runif(min = min(x2),max = max(x2),n=length(n)/2)
y3 <- runif(min = min(y2),max = max(y2),n=length(n)/2)
#assemble data into frame
X.trn <- matrix(c(x2,x3,y2,y3), nrow=3000, byrow=FALSE)
Y.trn <- matrix(c(rep(0,2000), rep(1,1000)), nrow=3000, byrow=FALSE)
nn = NNModel(input.dim=2, layers=c(5,5,5,1), activations=c('leaky.relu','leaky.relu','leaky.relu','sigmoid'))
nn.trn = train(nn,X.trn,Y.trn, epochs=50000, mini.batch.size=25, learning.rate=0.5)
prd_old = predict(nn.trn, matrix(c(min(X.trn[,1]),min(X.trn[,2]))))
# 3-layer multinomial, 6 clusters
#################################
library(MASS)
nn = NNModel(input.dim = 2, layers=c(5,5,6), activation=c('sigmoid','sigmoid','softmax'))
X.trn = rbind( mvrnorm(100, mu=c(0,.75), Sigma = diag(.1,nrow=2,ncol=2)),
mvrnorm(100, mu=c(.75,.5), Sigma = diag(.1,nrow=2,ncol=2)),
mvrnorm(100, mu=c(.75,-.5), Sigma = diag(.1,nrow=2,ncol=2)),
mvrnorm(100, mu=c(0,-.75), Sigma = diag(.1,nrow=2,ncol=2)),
mvrnorm(100, mu=c(-.75,-.5), Sigma = diag(.1,nrow=2,ncol=2)),
mvrnorm(100, mu=c(-.75,.5), Sigma = diag(.1,nrow=2,ncol=2)))
Y.trn = matrix( c(rep(c(1,0,0,0,0,0), 100),
rep(c(0,1,0,0,0,0), 100),
rep(c(0,0,1,0,0,0), 100),
rep(c(0,0,0,1,0,0), 100),
rep(c(0,0,0,0,1,0), 100),
rep(c(0,0,0,0,0,1), 100)), ncol=6, byrow=TRUE )
nn.trn = train(nn,X.trn,Y.trn, epochs=2500, mini.batch.size=15, learning.rate=0.1)
prd_old = predict(nn.trn, matrix(c(min(X.trn[,1]),min(X.trn[,2]))))
nn.trn
prd_old = predict(nn.trn, matrix(c(min(X.trn[,1]),min(X.trn[,2])),nrow=1))
prd_old
prd_old = which.max(predict(nn.trn, matrix(c(min(X.trn[,1]),min(X.trn[,2])),nrow=1)))
prd_old
#plot classes and decision boundary
plot(X.trn[,1], X.trn[,2], col=c(rep('red',100),rep('blue',100),rep('green',100),
rep('purple',100), rep('darkorange',100), rep('cyan',100)),
pch=19)
for( x1 in seq(from=min(X.trn[,1]), to=max(X.trn[,1]), length.out=200) ) {
for( x2 in seq(from=min(X.trn[,2]), to=max(X.trn[,2]), length.out=200) ) {
prd = predict(nn.trn, matrix(c(x1,x2), nrow=1))
clr = c('red', 'blue', 'green','purple','darkorange','cyan')[ which.max(prd) ]
points(x1,x2,pch=19,cex=.15,col=clr)
if( which.max(prd) != prd_old ){
points(x1,x2-0.5,pch=19,cex=0.15,col='black')
}
prd_old = which.max(prd)
}
}
seq(from=min(X.trn[,1]), to=max(X.trn[,1])
)
seq(from=min(X.trn[,1]), to=max(X.trn[,1]), length.out=200)
prd.old = which.max(predict(nn.trn, matrix(c(min(X.trn[,1]),min(X.trn[,2])),nrow=1)))
x2.old = min(X.trn[,2])
#plot classes and decision boundary
plot(X.trn[,1], X.trn[,2], col=c(rep('red',100),rep('blue',100),rep('green',100),
rep('purple',100), rep('darkorange',100), rep('cyan',100)),
pch=19)
prd.old = which.max(predict(nn.trn, matrix(c(min(X.trn[,1]),min(X.trn[,2])),nrow=1)))
x2.old = min(X.trn[,2])
x2.old
prd.old
for( x1 in seq(from=min(X.trn[,1]), to=max(X.trn[,1]), length.out=200) ) {
for( x2 in seq(from=min(X.trn[,2]), to=max(X.trn[,2]), length.out=200) ) {
prd = predict(nn.trn, matrix(c(x1,x2), nrow=1))
clr = c('red', 'blue', 'green','purple','darkorange','cyan')[ which.max(prd) ]
points(x1,x2,pch=19,cex=.15,col=clr)
if( which.max(prd) != prd.old ){
points(x1,(x2+x2.old)/2,pch=19,cex=0.15,col='black')
}
prd.old = which.max(prd)
x2.old = x2
}
}
#plot classes and decision boundary
plot(X.trn[,1], X.trn[,2], col=c(rep('red',100),rep('blue',100),rep('green',100),
rep('purple',100), rep('darkorange',100), rep('cyan',100)),
pch=19)
prd.old = which.max(predict(nn.trn, matrix(c(min(X.trn[,1]),min(X.trn[,2])),nrow=1)))
x2.old = min(X.trn[,2])
for( x1 in seq(from=min(X.trn[,1]), to=max(X.trn[,1]), length.out=200) ) {
for( x2 in seq(from=min(X.trn[,2]), to=max(X.trn[,2]), length.out=200) ) {
prd = predict(nn.trn, matrix(c(x1,x2), nrow=1))
clr = c('red', 'blue', 'green','purple','darkorange','cyan')[ which.max(prd) ]
points(x1,x2,pch=19,cex=.15,col=clr)
if( which.max(prd) != prd.old ){
points(x1,(x2+x2.old)/2,pch=19,cex=0.15,col='black')
}
prd.old = which.max(prd)
x2.old = x2
}
}
#plot classes and decision boundary
plot(X.trn[,1], X.trn[,2], col=c(rep('red',100),rep('blue',100),rep('green',100),
rep('purple',100), rep('darkorange',100), rep('cyan',100)),
pch=19)
prd.old = which.max(predict(nn.trn, matrix(c(min(X.trn[,1]),min(X.trn[,2])),nrow=1)))
x2.old = min(X.trn[,2])
prd.old = which.max(predict(nn.trn, matrix(c(min(X.trn[,1]),min(X.trn[,2])),nrow=1)))
x2.old = min(X.trn[,2])
for( x1 in seq(from=min(X.trn[,1]), to=max(X.trn[,1]), length.out=200) ) {
for( x2 in seq(from=min(X.trn[,2]), to=max(X.trn[,2]), length.out=200) ) {
prd = predict(nn.trn, matrix(c(x1,x2), nrow=1))
clr = c('red', 'blue', 'green','purple','darkorange','cyan')[ which.max(prd) ]
points(x1,x2,pch=19,cex=.15,col=clr)
if( which.max(prd) != prd.old & x2 != min(X.trn[,2]) ){
points(x1,(x2+x2.old)/2,pch=19,cex=0.15,col='black')
}
prd.old = which.max(prd)
x2.old = x2
}
}
#plot classes and decision boundary
plot(X.trn[,1], X.trn[,2], col=c(rep('red',100),rep('blue',100),rep('green',100),
rep('purple',100), rep('darkorange',100), rep('cyan',100)),
pch=19)
prd.old = which.max(predict(nn.trn, matrix(c(min(X.trn[,1]),min(X.trn[,2])),nrow=1)))
x2.old = min(X.trn[,2])
for( x1 in seq(from=min(X.trn[,1]), to=max(X.trn[,1]), length.out=200) ) {
for( x2 in seq(from=min(X.trn[,2]), to=max(X.trn[,2]), length.out=200) ) {
prd = predict(nn.trn, matrix(c(x1,x2), nrow=1))
clr = c('red', 'blue', 'green','purple','darkorange','cyan')[ which.max(prd) ]
points(x1,x2,pch=19,cex=.15,col=clr)
if( which.max(prd) != prd.old & x2 != min(X.trn[,2]) ){
points(x1,(x2+x2.old)/2,pch=19,cex=0.15,col='black')
}
prd.old = which.max(prd)
x2.old = x2
}
}
# 3-layer multinomial, 10 randomly placed clusters
##################################################
library(MASS)
nn = NNModel(input.dim = 2, layers=c(5,5,10), activation=c('sigmoid','sigmoid','softmax'),seed=180930)
x1.rnd = runif(10,-1,1)
x2.rnd = runif(10,-1,1)
s.rnd = rnorm(n=10,mean=0.1,sd=.05)
X.trn = rbind( mvrnorm(100, mu=c(x1.rnd[1], x2.rnd[1]), Sigma = diag(s.rnd[1],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[2], x2.rnd[2]), Sigma = diag(s.rnd[2],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[3], x2.rnd[3]), Sigma = diag(s.rnd[3],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[4], x2.rnd[4]), Sigma = diag(s.rnd[4],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[5], x2.rnd[5]), Sigma = diag(s.rnd[5],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[6], x2.rnd[6]), Sigma = diag(s.rnd[6],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[7], x2.rnd[7]), Sigma = diag(s.rnd[7],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[8], x2.rnd[8]), Sigma = diag(s.rnd[8],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[9], x2.rnd[9]), Sigma = diag(s.rnd[9],nrow=2,ncol=2)),
mvrnorm(100, mu=c(x1.rnd[10], x2.rnd[10]), Sigma = diag(s.rnd[10],nrow=2,ncol=2)))
Y.trn = matrix( c(rep(c(1,0,0,0,0,0,0,0,0,0), 100),
rep(c(0,1,0,0,0,0,0,0,0,0), 100),
rep(c(0,0,1,0,0,0,0,0,0,0), 100),
rep(c(0,0,0,1,0,0,0,0,0,0), 100),
rep(c(0,0,0,0,1,0,0,0,0,0), 100),
rep(c(0,0,0,0,0,1,0,0,0,0), 100),
rep(c(0,0,0,0,0,0,1,0,0,0), 100),
rep(c(0,0,0,0,0,0,0,1,0,0), 100),
rep(c(0,0,0,0,0,0,0,0,1,0), 100),
rep(c(0,0,0,0,0,0,0,0,0,1), 100)), ncol=10, byrow=TRUE )
nn.trn = train(nn,X.trn,Y.trn, epochs=1000, mini.batch.size=25, learning.rate=0.1)
nn.prd = predict(nn.trn, X.trn)
Y.trn - nn.prd
#plot classes and decision boundary
plot(X.trn[,1], X.trn[,2], col=rep(rainbow(10),each=100), pch=19)
for( x1 in seq(from=min(X.trn[,1]), to=max(X.trn[,1]), length.out=200) ) {
for( x2 in seq(from=min(X.trn[,2]), to=max(X.trn[,2]), length.out=200) ){
prd = predict(nn.trn, matrix(c(x1,x2), nrow=1))
clr = rainbow(10)[ which.max(prd) ]
points(x1,x2,pch=19,cex=.15,col=clr)
}
}
#plot classes and decision boundary
plot(X.trn[,1], X.trn[,2], col=rep(rainbow(10),each=100), pch=19)
prd.old = which.max(predict(nn.trn, matrix(c(min(X.trn[,1]),min(X.trn[,2])),nrow=1)))
x2.old = min(X.trn[,2])
for( x1 in seq(from=min(X.trn[,1]), to=max(X.trn[,1]), length.out=200) ) {
for( x2 in seq(from=min(X.trn[,2]), to=max(X.trn[,2]), length.out=200) ) {
prd = predict(nn.trn, matrix(c(x1,x2), nrow=1))
clr = c('red', 'blue', 'green','purple','darkorange','cyan')[ which.max(prd) ]
points(x1,x2,pch=19,cex=.15,col=clr)
if( which.max(prd) != prd.old & x2 != min(X.trn[,2]) ){
points(x1,(x2+x2.old)/2,pch=19,cex=0.15,col='black')
}
prd.old = which.max(prd)
x2.old = x2
}
}
#plot classes and decision boundary
plot(X.trn[,1], X.trn[,2], col=rep(rainbow(10),each=100), pch=19)
prd.old = which.max(predict(nn.trn, matrix(c(min(X.trn[,1]),min(X.trn[,2])),nrow=1)))
x2.old = min(X.trn[,2])
for( x1 in seq(from=min(X.trn[,1]), to=max(X.trn[,1]), length.out=200) ) {
for( x2 in seq(from=min(X.trn[,2]), to=max(X.trn[,2]), length.out=200) ) {
prd = predict(nn.trn, matrix(c(x1,x2), nrow=1))
clr = rainbow(10)[ which.max(prd) ]
points(x1,x2,pch=19,cex=.15,col=clr)
if( which.max(prd) != prd.old & x2 != min(X.trn[,2]) ){
points(x1,(x2+x2.old)/2,pch=19,cex=0.15,col='black')
}
prd.old = which.max(prd)
x2.old = x2
}
}

# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 2:(length(layers)+1), function(l) {
activation = activations[l]
if( l == 2){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = length(input.dim) )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
})
names(nn$layers) = paste('L', 2:(length(layers)+1), sep='')
nn
}
z
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
activation = activations[l]
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = length(input.dim) )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
})
names(nn$layers) = paste('L', 2:(length(layers)+1), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
activation = activations[l]
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = length(input.dim) )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
})
names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
activation = activations[l]
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = input.di) )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
})
names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
activation = activations[l]
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = input.dim )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
})
names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
activation = activations[l]
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = input.dim )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
})
#names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = input.dim )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
c( activation = activations[l],
weights = weights)
})
#names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = input.dim )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
list( activation = activations[l],
weights = weights)
})
#names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
NNModel = function( input.dim = NULL, layers = NULL, activations = NULL,
batch.norm = NULL, drop.out.rate = NULL,
learning.rate = 0.01 ) {
# input.dim = dimensions of input
# layers = vector of number of nodes per hidden/output layer
# activations = vector of activations types per layer
# batch.nrom = vector of TRUE/FALSE designating whether batch norm is performed for each layer
# drop.out.rate = vector of values for drop-out rate per layer
# assumed all vectors same length = number of layers
nn = list( layers = list(), learning.rate = learning.rate)
nn$layers = lapply( 1:(length(layers)), function(l) {
if( l == 1){
weights = matrix( rnorm( input.dim*layers[l], 0, 0.01),
nrow = input.dim )
}else{
weights = matrix( rnorm(layers[l-1] * layers[l], 0, 0.01),
nrow = layers[l-1] )
}
list( activation = activations[l],
weights = weights)
})
names(nn$layers) = paste('L', 1:(length(layers)), sep='')
nn
}
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
z = NNModel(input.dim = 100, layers = c(50,10,10,1), activations = c('sigmoid', 'sigmoid', sigmoid', 'softmax'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z = NNModel(input.dim = 100, layers = c(50,10,10,1), activations = c('sigmoid', 'sigmoid', 'sigmoid', 'softmax'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
z = NNModel(input.dim = 10, layers = c(5,1), activations = c('sigmoid', 'linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
x=rep(1,10)
x
a = forward.prop(NNModel, x)
a = forward.prop(z, x)
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
a = forward.prop(z, x)
x=matrix(10,x,nrow=10)
x
a = forward.prop(z, x)
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
a = forward.prop(z, x)
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
a = forward.prop(z, x)
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
a = forward.prop(z, x)
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
a = forward.prop(z, x)
source('/storage/ColoComp/Development/NN/NNfromscratch.R')
z
x
a
z$layers$L1$weights[1,]
z$layers$L1$weights[1,]*10
sum(z$layers$L1$weights[1,]*10)
sigmoid(-0.2845585)
sum(z$layers$L1$weights[,1]*10)
z$layers$L1$weights[,1]
z$layers$L1$weights[,1]*10
sum(z$layers$L1$weights[,1]*10)
sigmoid(sum(z$layers$L1$weights[,1]*10))
z = NNModel(input.dim = 10, layers = c(1), activations = c('linear'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
a = forward.prop(z, x)
a
x
x=matrix(x,nrow=10)
x
x=matrix(rep(1,10),nrow=10)
x
a = forward.prop(z, x)
a
z
z
a
sum(z$layers$L1$weights)
z = NNModel(input.dim = 10, layers = c(1), activations = c('sigmoid'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
a = forward.prop(z, x)
a
tanh(sum(z$layers$L1$weights))
a
z
a
x
tanh(sum(z$layers$L1$weights))
a
z = NNModel(input.dim = 10, layers = c(1), activations = c('tanh'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
a = forward.prop(z, x)
a
tanh(sum(z$layers$L1$weights))
z = NNModel(input.dim = 10, layers = c(1), activations = c('relu'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
a = forward.prop(z, x)
a
(sum(z$layers$L1$weights))
z = NNModel(input.dim = 10, layers = c(1), activations = c('relu'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
(sum(z$layers$L1$weights))
z = NNModel(input.dim = 10, layers = c(1), activations = c('relu'), batch.norm=FALSE, drop.out.rate = 0, learning.rate = 0.01)
z
(sum(z$layers$L1$weights))
a
a = forward.prop(z, x)
a
setwd("/storage/ColoComp/Development/NNFS")
source('/storage/ColoComp/Development/NNFS/NNFS.R')
nn = NNModel(input.dim=3, layers=c(3,1), activations=c('linear','linear'), learning.rate=0.5)
nn
source('/storage/ColoComp/Development/NNFS/NNFS.R')
X.trn = matrix(rnorm(200,0,1), nrow=100)
Y.trn = matrix(5 * X.trn[,1] - 7.25 * X.trn[, 2] + 6.83 * X.trn[, 3], nrow=100)
X.trn = matrix(rnorm(300,0,1), nrow=100)
Y.trn = matrix(5 * X.trn[,1] - 7.25 * X.trn[, 2] + 6.83 * X.trn[, 3], nrow=100)
X.trn
YK.trn
Y.trn
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
traceback()
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
NNmod$layers[[layers[l-1]]]$a
NNmod$layers[[layers[l]]]$weights
NNmod$layers[[layers[l-1]]]$weights
source('/storage/ColoComp/Development/NNFS/NNFS.R')
source('/storage/ColoComp/Development/NNFS/NNFS.R')
nn
nn.fp = forward.prop(nn,X.trn)
nn.fp
nn$layers$L1$weights
nn$layers$L1$weights = matrix(c(1,1,1,2,2,2,3,3,3), nrow=3)
nn$layers$L1$weights
nn$layers$L2$weights
nn$layers$L2$weights = matrix(c(1,1,1), nrow=3)
nn
X.trn = matrix( c(5,5,5), nrow=1)
X.trn
nn.fp = forward.prop(nn,X.trn)
nn.fp
X.trn
x.trn = matrix( rep(c(5,5,5),100), nrow=100)
X.trn = matrix( rep(c(5,5,5),100), nrow=100)
rm(x.trn)
X.trn
Y.trn = forward.prop(nn,X.trn )
Y.trn
X.trn = matrix(rnorm(300,0,1), nrow=100)
Y.trn = forward.prop(nn,X.trn )
Y.trn
nn
nn.orig = nn
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
traceback()
X.trn
Y.trn
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
mini.batch.X
Y.trn
nn.out = forward.prop(nn,X.trn )
nn.out$layers$L2$a
Y.trn = nn.out$layers$L2$a
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
delta
d()
d.linear()
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
l
Y.trn
NNmod$layers[[layer]]$a
nn
nn = NNModel(input.dim=3, layers=c(3,1), activations=c('linear','linear'), learning.rate=0.5)
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
Y.trn
NNmod$layers[[layer]]$a
delta
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn = NNModel(input.dim=3, layers=1, activations='linear', learning.rate=0.1)
X.trn = matrix(rnorm(300,0,1), nrow=100)
Y.trn = matrix(5 * X.trn[,1] - 7.25 * X.trn[, 2] + 6.83 * X.trn[, 3], nrow=100)
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
nn.trn
nn.trn = train(nn,X.trn,Y.trn, epochs=20, mini.batch.size=15)
nn.trn
# 2 layer, linear
nn.orig = NNModel(input.dim=3, layers=c(3,1), activations=c('linear','linear'), learning.rate=0.5)
nn$layers$L1$weights = matrix(c(1,1,1,2,2,2,3,3,3), nrow=3)
nn.orig$layers$L1$weights = matrix(c(1,1,1,2,2,2,3,3,3), nrow=3)
nn.orig$layers$L2$weights = matrix(c(1,1,1), nrow=3)
nn.orig
X.trn = matrix(rnorm(300,0,1), nrow=100)
nn.fp = forward.prop(nn,X.trn)
nn.fp
# 2 layer, linear
nn.orig = NNModel(input.dim=3, layers=c(3,1), activations=c('linear','linear'), learning.rate=0.5)
nn.orig$layers$L1$weights = matrix(c(1,1,1,2,2,2,3,3,3), nrow=3)
nn.orig$layers$L2$weights = matrix(c(1,1,1), nrow=3)
nn.fp = forward.prop(nn.orig,X.trn)
nn.fp
Y.trn = nn.fp$layers$L2$z
nn = NNModel(input.dim=3, layers=c(3,1), activations=c('linear','linear'), learning.rate=0.5)
nn
source('/storage/ColoComp/Development/NNFS/NNFS.R')
source('/storage/ColoComp/Development/NNFS/NNFS.R')
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
layers[l-1]
NNmod$layers[[prev.layer]]$z
delta
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
grad
zi
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
zi
NNmod$layers[[prev.layer]]$z
NNmod$layers[[prev.layer]]$z
NNmod$layers[[layer]]$z
delta
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
layers
l
layer
delta
X.trn
NNmod$layers[[prev.layer]]$z
NNmod$layers[[prev.layer]]$z
NNmod$layers[[prev.layer]]$z
delta
zi
grad
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
traceback()
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
traceback()
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
grad
avg.grad
source('/storage/ColoComp/Development/NNFS/NNFS.R')
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
source('/storage/ColoComp/Development/NNFS/NNFS.R')
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
avg.grad
NNmod$layers[[layer]]$weights
NNmod$layers[[layer]]$weights
l
layer
delta
NNmod$layers[[layer]]$weights
source('/storage/ColoComp/Development/NNFS/NNFS.R')
# Linear regression, single layer
nn = NNModel(input.dim=3, layers=1, activations='linear', learning.rate=0.1)
X.trn = matrix(rnorm(300,0,1), nrow=100)
Y.trn = matrix(5 * X.trn[,1] - 7.25 * X.trn[, 2] + 6.83 * X.trn[, 3], nrow=100)
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
nn.trn
# 2 layer, linear
nn.orig = NNModel(input.dim=3, layers=c(3,1), activations=c('linear','linear'), learning.rate=0.5)
nn.orig$layers$L1$weights = matrix(c(1,1,1,2,2,2,3,3,3), nrow=3)
nn.orig$layers$L2$weights = matrix(c(1,1,1), nrow=3)
X.trn = matrix(rnorm(300,0,1), nrow=100)
nn.fp = forward.prop(nn.orig,X.trn)
Y.trn = nn.fp$layers$L2$z
nn = NNModel(input.dim=3, layers=c(3,1), activations=c('linear','linear'), learning.rate=0.5)
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
NNmod
NNmod
NNmod
NNmod
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
debugSource('/storage/ColoComp/Development/NNFS/NNFS.R')
nn.trn = train(nn,X.trn,Y.trn, epochs=10, mini.batch.size=15)
NNmod
NNmod$layers$L1$weights
X.trn[1,]
X.trn[1, ] %*% NNmod$layers$L1$weights
mini.batch.X[1, ]
mini.batch.X[1, ] %*% NNmod$layers$L1$weights
NNmod$layers$L1$a
NNmod$layers$L2$weights
NNmod$layers$L1$a[1, ] %*% NNmod$layers$L2$weights
